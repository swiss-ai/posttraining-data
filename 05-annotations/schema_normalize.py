#!/usr/bin/env python3
"""
Schema Normalization Script for New Chat Format Datasets

This script applies a unified schema to datasets with inconsistent schemas,
ensuring all samples have the same field structure and types. This enables
parallel processing with dataset.map() without Arrow casting errors.

Uses schema files generated by schema_extract.py.
"""

import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from datasets import load_from_disk, Dataset, DatasetDict
from tqdm import tqdm


def load_unified_schema(schema_file: Path) -> Dict[str, str]:
    """
    Load unified schema from JSON file.
    
    Args:
        schema_file: Path to schema_unified.json file
        
    Returns:
        Dictionary mapping field paths to types
    """
    if not schema_file.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_file}")
    
    with open(schema_file, 'r') as f:
        return json.load(f)


def get_default_value_for_type(field_type: str) -> Any:
    """
    Get appropriate default value for a field type.
    
    Args:
        field_type: Type string from schema
        
    Returns:
        Default value for the type
    """
    if field_type == "null":
        return None
    elif field_type == "string":
        return None  # Keep as None to avoid forcing empty strings
    elif field_type == "number":
        return None  # Keep as None to avoid forcing zeros
    elif field_type == "boolean":
        return None  # Keep as None to avoid forcing False
    elif field_type == "list" or field_type == "empty_list":
        return []
    elif field_type == "dict" or field_type == "empty_dict":
        return {}
    else:
        return None


def set_nested_field(obj: Dict[str, Any], path: str, value: Any) -> None:
    """
    Set a nested field in an object using dot notation.
    
    Args:
        obj: Object to modify
        path: Dot-separated path (e.g., 'metadata.keyword_detection')
        value: Value to set
    """
    parts = path.split('.')
    current = obj
    
    # Navigate to parent of target field
    for part in parts[:-1]:
        # Handle list indices like [0], [1], etc.
        if '[' in part and ']' in part:
            field_name = part.split('[')[0]
            index = int(part.split('[')[1].split(']')[0])
            
            # Ensure field exists and is a list
            if field_name not in current:
                current[field_name] = []
            
            # Extend list if necessary
            while len(current[field_name]) <= index:
                current[field_name].append({})
            
            current = current[field_name][index]
        else:
            # Regular field access
            if part not in current:
                current[part] = {}
            current = current[part]
    
    # Set the final field
    final_field = parts[-1]
    if '[' in final_field and ']' in final_field:
        field_name = final_field.split('[')[0]
        index = int(final_field.split('[')[1].split(']')[0])
        
        if field_name not in current:
            current[field_name] = []
        
        while len(current[field_name]) <= index:
            current[field_name].append(None)
        
        current[field_name][index] = value
    else:
        current[final_field] = value


def get_nested_field(obj: Dict[str, Any], path: str) -> Any:
    """
    Get a nested field from an object using dot notation.
    
    Args:
        obj: Object to read from
        path: Dot-separated path
        
    Returns:
        Field value or None if not found
    """
    parts = path.split('.')
    current = obj
    
    try:
        for part in parts:
            if '[' in part and ']' in part:
                field_name = part.split('[')[0]
                index = int(part.split('[')[1].split(']')[0])
                current = current[field_name][index]
            else:
                current = current[part]
        return current
    except (KeyError, IndexError, TypeError):
        return None


def normalize_sample_schema(sample: Dict[str, Any], unified_schema: Dict[str, str]) -> Dict[str, Any]:
    """
    Normalize a single sample to match unified schema.
    
    Simple approach: ensure all nested structures exist, but don't force field values.
    This prevents Arrow casting errors while preserving original data.
    
    Args:
        sample: Sample to normalize
        unified_schema: Unified schema with all possible fields
        
    Returns:
        Normalized sample
    """
    import copy
    normalized = copy.deepcopy(sample)
    
    # Ensure all required nested structures exist
    # Focus on metadata dictionaries which are the main source of inconsistencies
    
    # Ensure system_prompt.metadata exists
    if "system_prompt" in normalized and normalized["system_prompt"]:
        if "metadata" not in normalized["system_prompt"]:
            normalized["system_prompt"]["metadata"] = {}
    
    # Ensure initial_prompt.metadata exists  
    if "initial_prompt" in normalized and normalized["initial_prompt"]:
        if "metadata" not in normalized["initial_prompt"]:
            normalized["initial_prompt"]["metadata"] = {}
    
    # Ensure conversation_branches message parts have metadata
    if "conversation_branches" in normalized:
        for branch in normalized["conversation_branches"]:
            if "messages" in branch:
                for message in branch["messages"]:
                    if "parts" in message:
                        for part in message["parts"]:
                            if "metadata" not in part:
                                part["metadata"] = {}
    
    return normalized


def normalize_batch_schema(examples: Dict[str, List], unified_schema: Dict[str, str]) -> Dict[str, List]:
    """
    Normalize a batch of samples for dataset.map().
    
    Args:
        examples: Batch of examples
        unified_schema: Unified schema
        
    Returns:
        Normalized batch
    """
    import copy
    result = copy.deepcopy(examples)
    
    batch_size = len(examples[next(iter(examples))])
    
    for idx in range(batch_size):
        # Extract sample from batch
        sample = {key: result[key][idx] for key in result}
        
        # Normalize sample
        normalized_sample = normalize_sample_schema(sample, unified_schema)
        
        # Put back into batch
        for key in result:
            result[key][idx] = normalized_sample[key]
    
    return result


def normalize_dataset(dataset: Dataset, unified_schema: Dict[str, str], 
                     num_proc: int = None, batch_size: int = 1000) -> Dataset:
    """
    Normalize entire dataset using unified schema.
    
    Args:
        dataset: Input dataset
        unified_schema: Unified schema to apply
        num_proc: Number of processes for parallel processing
        batch_size: Batch size for processing
        
    Returns:
        Normalized dataset
    """
    print(f"Normalizing dataset with unified schema...")
    print(f"Dataset size: {len(dataset):,} samples")
    print(f"Unified schema fields: {len(unified_schema)}")
    print(f"Using {num_proc or 'auto'} processes, batch size {batch_size:,}")
    
    normalized_dataset = dataset.map(
        lambda examples: normalize_batch_schema(examples, unified_schema),
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        load_from_cache_file=False,
        desc="Normalizing schema"
    )
    
    print(f"Normalization complete: {len(normalized_dataset):,} samples")
    return normalized_dataset


def save_normalized_dataset(dataset: Dataset, output_path: Path, 
                          input_path: Path, schema_dir: Path):
    """
    Save normalized dataset with metadata.
    
    Args:
        dataset: Normalized dataset
        output_path: Output directory
        input_path: Original dataset path
        schema_dir: Schema directory used for normalization
    """
    # Ensure output directory exists
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Save dataset
    print(f"Saving normalized dataset to {output_path}...")
    dataset.save_to_disk(str(output_path))
    
    # Create metadata
    metadata = {
        "operation": "schema_normalization",
        "script": "schema_normalize.py",
        "timestamp": datetime.now().isoformat(),
        "input_path": str(input_path),
        "output_path": str(output_path),
        "schema_directory": str(schema_dir),
        "normalized_samples": len(dataset),
        "normalization_success": True,
        "format": "new_chat_format_with_parts"
    }
    
    # Save metadata
    metadata_file = output_path / "dataset_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump({"processing_log": [metadata]}, f, indent=2)
    print(f"Metadata saved to {metadata_file}")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Normalize dataset schema using unified schema from schema extraction",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Normalize dataset using extracted schemas
  venv/bin/python 05-annotations/schema_normalize.py \\
    data/02-standardised-newformat/dataset \\
    data/02-standardised-newformat/dataset-normalized \\
    --schema-dir schemas/
  
  # With custom processing settings
  venv/bin/python 05-annotations/schema_normalize.py \\
    data/path/input \\
    data/path/output \\
    --schema-dir schemas/ \\
    --num-proc 16 \\
    --batch-size 5000
  
  # Single process for debugging
  venv/bin/python 05-annotations/schema_normalize.py \\
    data/path/input \\
    data/path/output \\
    --schema-dir schemas/ \\
    --num-proc 1
        """
    )
    
    parser.add_argument(
        "input_path",
        help="Path to input dataset directory"
    )
    parser.add_argument(
        "output_path",
        help="Path for output normalized dataset"
    )
    parser.add_argument(
        "--schema-dir",
        required=True,
        help="Directory containing schema files from schema_extract.py"
    )
    parser.add_argument(
        "--num-proc",
        type=int,
        default=None,
        help="Number of processes for parallel processing (default: auto)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="Batch size for processing (default: 1000)"
    )
    
    return parser.parse_args()


def main():
    """Main normalization function."""
    args = parse_arguments()
    
    # Validate paths
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"Error: Input path does not exist: {input_path}")
        sys.exit(1)
    
    schema_dir = Path(args.schema_dir)
    if not schema_dir.exists():
        print(f"Error: Schema directory does not exist: {schema_dir}")
        sys.exit(1)
    
    unified_schema_file = schema_dir / "schema_unified.json"
    if not unified_schema_file.exists():
        print(f"Error: Unified schema file not found: {unified_schema_file}")
        print(f"Run schema_extract.py first to generate schema files.")
        sys.exit(1)
    
    output_path = Path(args.output_path)
    
    # Load unified schema
    print(f"Loading unified schema from: {unified_schema_file}")
    try:
        unified_schema = load_unified_schema(unified_schema_file)
        print(f"Loaded schema with {len(unified_schema)} fields")
    except Exception as e:
        print(f"Error loading unified schema: {e}")
        sys.exit(1)
    
    # Load dataset
    print(f"Loading dataset from: {input_path}")
    try:
        dataset = load_from_disk(str(input_path))
        
        # Handle DatasetDict vs single Dataset
        if isinstance(dataset, DatasetDict):
            available_splits = list(dataset.keys())
            print(f"Found DatasetDict with splits: {available_splits}")
            
            if 'train' in available_splits:
                dataset = dataset['train']
                print(f"Using 'train' split")
            else:
                first_split = available_splits[0]
                dataset = dataset[first_split]
                print(f"Using '{first_split}' split")
        
        print(f"Dataset size: {len(dataset):,} samples")
        
    except Exception as e:
        print(f"Error loading dataset: {e}")
        sys.exit(1)
    
    # Normalize dataset
    try:
        normalized_dataset = normalize_dataset(
            dataset,
            unified_schema,
            num_proc=args.num_proc,
            batch_size=args.batch_size
        )
    except Exception as e:
        print(f"Error during normalization: {e}")
        sys.exit(1)
    
    # Save normalized dataset
    save_normalized_dataset(normalized_dataset, output_path, input_path, schema_dir)
    
    print(f"\n✅ Schema normalization completed!")
    print(f"Input:  {input_path}")
    print(f"Output: {output_path}")
    print(f"Samples: {len(normalized_dataset):,}")
    print(f"Schema: {unified_schema_file}")
    print(f"\n🚀 Dataset is now ready for parallel processing with dataset.map()")


if __name__ == "__main__":
    main()